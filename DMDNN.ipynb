{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorboard\n",
    "import tensorflow.keras as K\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras import backend\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import activations\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.regularizers import l1\n",
    "from tensorflow.keras.models import model_from_json\n",
    "import scipy.io\n",
    "import h5py as h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Metrics [Can be changed]\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "# Percentage error: AMPE for one output\n",
    "def AMPE(y_true, y_pred):  \n",
    "    return 100*K.abs(K.mean((y_pred-y_true)/(1+y_true)))\n",
    "    #return AME(y_true, y_pred)\n",
    "\n",
    "# Percentage error: AMPE_all for all outputs\n",
    "# y= [.1y_12, .2y_18, .3y_24, .4y_30, .5y_36]\n",
    "def AMPE_all(y_true, y_pred):\n",
    "    loss = AMPE(y_true[:,0],y_pred[:,0])\n",
    "    for i in np.arange(y_pred.shape[1]):\n",
    "        loss = loss + AMPE(y_true[:,i],y_pred[:,i])\n",
    "    return loss\n",
    "\n",
    "# Weighted MSE\n",
    "def WMSE(y_true, y_pred):\n",
    "    loss = K.mean(K.square((y_pred[:,0]-y_true[:,0])))\n",
    "    for i in np.arange(y_pred.shape[1]):\n",
    "        loss = loss + (i+1)*K.mean(K.square((y_pred[:,i]-y_true[:,i])))\n",
    "    return loss\n",
    "\n",
    "# Centered Percentage error: CMAPE\n",
    "def CMAPE(y_true, y_pred):\n",
    "    return K.sqrt(WMSE(y_true,y_pred)) + .1*AMPE_all(y_true,y_pred)\n",
    "\n",
    "# RMSE at 36\n",
    "def RMSE_36(y_true, y_pred):\n",
    "    return K.sqrt(K.mean(K.square((y_pred[:,7]-y_true[:,7]))))\n",
    "\n",
    "# AMPE at 36\n",
    "def AMPE_36(y_true, y_pred):\n",
    "    return AMPE(y_true[:,4],y_pred[:,7])\n",
    "\n",
    "\n",
    "# Weighted MAE\n",
    "def WMAE(y_true, y_pred):\n",
    "    loss = K.mean(K.abs((y_pred[:,0]-y_true[:,0])))\n",
    "    for i in np.arange(y_pred.shape[1]):\n",
    "        loss = loss + (i+1)*K.mean(K.square((y_pred[:,i]-y_true[:,i])))\n",
    "    return loss\n",
    "\n",
    "# MSE\n",
    "def WMSE_yichao(y_true, y_pred):\n",
    "    for i in np.arange(y_pred.shape[1]):\n",
    "        if i == 0:\n",
    "            loss = K.mean(K.square((y_pred[:,0]-y_true[:,0])))\n",
    "        else:\n",
    "            loss = loss + K.mean(K.square((y_pred[:,i]-y_true[:,i])))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A way to look at the epoch results\n",
    "class NBatchLogger(tf.keras.callbacks.Callback):\n",
    "\n",
    "    def __init__(self, display):\n",
    "        self.step = 0\n",
    "        self.display = display\n",
    "        self.metric_cache = {}\n",
    "    \n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        self.step += 1\n",
    "        for k in logs.keys():\n",
    "            if k in logs:\n",
    "                self.metric_cache[k] = self.metric_cache.get(k, 0) + logs[k]\n",
    "        if self.step % self.display == 0:\n",
    "            metrics_log = ''\n",
    "            for (k, v) in self.metric_cache.items():\n",
    "                val = v / self.display\n",
    "                if abs(val) > 1e-3:\n",
    "                    metrics_log += ' - %s: %.1f' % (k, val)\n",
    "                else:\n",
    "                    metrics_log += ' - %s: %.4e' % (k, val)\n",
    "            print('Epoch {}/{}{}'.format(self.step,\n",
    "                                          self.params['epochs'],\n",
    "                                          metrics_log))            \n",
    "            self.metric_cache.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks while training the neural network\n",
    "\n",
    "# 1. Stop when the validation loss is not improving [give a buffer of 100 epochs]\n",
    "# Restore the best weights with the best validation loss when the training is over\n",
    "earlyStop = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    patience=100,\n",
    "    restore_best_weights=True)\n",
    "\n",
    "# 2. Tensorboard validation and training performance\n",
    "plotLogs = tf.keras.callbacks.TensorBoard(log_dir=\"./logs-chehade2/\", histogram_freq=0, write_graph=False, write_images=True,profile_batch = 100000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "import scipy.io\n",
    "mat = scipy.io.loadmat('ECE5831_dataset.mat')\n",
    "X_tr = mat['X_tr']\n",
    "X_tst = mat['X_tst']\n",
    "Y_tr = mat['Y_tr']\n",
    "Y_tst = mat['Y_tst']\n",
    "\n",
    "\n",
    "X_tst = X_tst[(Y_tst[:,4] < 2000),:]\n",
    "# print(X_tst.shape)\n",
    "Y_tst = Y_tst[(Y_tst[:,4] < 2000),:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Deep Autoencded Warranty Features\n",
    "nn_dest = 'Models_2022'\n",
    "\n",
    "from random import randrange\n",
    "import matplotlib.pyplot as plt\n",
    "import io\n",
    "\n",
    "# Y_tr_0 = Y_tr/(.001+X_tr[:,18].reshape(X_tr.shape[0],1))\n",
    "# X_tr_0 = X_tr[:,:18]/(.001+X_tr[:,18].reshape(X_tr.shape[0],1))\n",
    "\n",
    "# Y_tst_0 = Y_tst/(.001+X_tst[:,18].reshape(X_tst.shape[0],1))\n",
    "# X_tst_0 = X_tst[:,:18]/(.001+X_tst[:,18].reshape(X_tst.shape[0],1))\n",
    "\n",
    "Y_tr_0 = Y_tr.copy()\n",
    "X_tr_0 = X_tr[:,:19]\n",
    "Y_tst_0 = Y_tst.copy()\n",
    "X_tst_0 = X_tst[:,:19]\n",
    "\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "\n",
    "# Define a custom layer for using the neurones of a hidden layer as parameters of the Weibull CDF or general CDF\n",
    "class Lay(layers.Layer):\n",
    "    \n",
    "    # Standard function\n",
    "    def init(self):\n",
    "        super(Lay,self).__init__()\n",
    "        \n",
    "    # Standard function\n",
    "    def build(self,inputShape):\n",
    "        super(Lay,self).build(inputShape)\n",
    "        \n",
    "    # Custom function\n",
    "    def call(self,x):\n",
    "        \n",
    "        # Saturate the parameters of the Weibull CDF to avoid unstable solutions\n",
    "        x = K.relu(x, max_value=1000)\n",
    "        \n",
    "        # Return the Weibull CDF estimates as survival estimates [bounded between 0 and 1]\n",
    "        # First given neuron x[:,0:1] is the scale parameter\n",
    "        # Second given neuron x[:,1:2] is the shape parameter\n",
    "        # Thus, the input layer to the our custom layer should have at least two neurons [best to be exactly two neurons]\n",
    "        return (1-K.exp(-K.pow((K.constant([[1/36,1/36,2/36,13/36,19/36,25/36,31/36,37/36]]))*x[:,0:1],x[:,1:2])))\n",
    "\n",
    "\n",
    "from random import randrange\n",
    "import matplotlib.pyplot as plt\n",
    "import io\n",
    "\n",
    "\n",
    "# Create an empty prediction matrix to store the predictions from the \"N\" neural networks of maturation level \"t\"\n",
    "N = 15 # 15 networks with different with initializations\n",
    "Y_tr_pred = np.empty([Y_tr_0.shape[0], Y_tr_0.shape[1], N])\n",
    "\n",
    "# Start training networks for maturation level \"t\"\n",
    "for j in np.arange(N):\n",
    "\n",
    "    # Construct names for the weights and model file names\n",
    "    w_name = nn_dest + '\\\\DNN_weights_'  + '_' + str(j) + '.h5'\n",
    "    mdl_name = nn_dest + '\\\\DNN_'  + '_' + str(j) + '.json'\n",
    "\n",
    "    # Create the neural network architecture\n",
    "    input_tensor = layers.Input(shape=(X_tr_0.shape[1],))\n",
    "\n",
    "    h1 = layers.Dense(units=128, activation='relu')(input_tensor)\n",
    "    #h1 = layers.Dropout(0.2)(h1)\n",
    "    h2 = layers.Dense(units=64, activation='relu')(h1)\n",
    "    #h2 = layers.Dropout(0.2)(h2)\n",
    "    h3 = layers.Dense(units=32, activation='relu')(h2)\n",
    "    #h3 = layers.Dropout(0.2)(h3)\n",
    "\n",
    "    # Define a custom output = Sigmoid(h3 nodes) * Linear (h3 nodes)\n",
    "    # This is equivalent of \n",
    "    y11 = layers.Dense(units=Y_tr_0.shape[1], activation='sigmoid')(h3)\n",
    "    y12 = layers.Dense(units=Y_tr_0.shape[1], activation='linear')(h3)\n",
    "\n",
    "    output_tensor2 = layers.Multiply()([y11,y12])    \n",
    "\n",
    "    # Define the network inputs and outputs\n",
    "    network = models.Model(inputs=[input_tensor],outputs=[output_tensor2])\n",
    "\n",
    "    # Compile the model\n",
    "    network.compile(loss=WMSE_yichao,\n",
    "                #optimizer=tf.keras.optimizers.Adam(learning_rate=0.01), # Optimization algorithm\n",
    "                #optimizer = tf.optimizers.Adam(amsgrad=True),\n",
    "                optimizer = 'Adam',\n",
    "                #metrics=[AME,AMPE,MAPE,RMSE,RMSE_36,INCR_trend]) \n",
    "                metrics=[RMSE_36,AMPE_36]) \n",
    "\n",
    "    # Train the model\n",
    "    out_batch = NBatchLogger(display=10)\n",
    "    history = network.fit(X_tr_0,# Features\n",
    "                    Y_tr_0, # Target vector\n",
    "                    epochs=10000, # Number of epochs\n",
    "                    verbose=0, # No output\n",
    "                    batch_size=np.ceil(Y_tr_0.shape[0]/10).astype(int), # Number of observations per batch\n",
    "                    #batch_size = 16,\n",
    "                    validation_split = 0.3,\n",
    "                    callbacks=[out_batch, earlyStop])#, plotLogs, plot_fig])\n",
    "\n",
    "    # Save the model\n",
    "    # serialize model to JSON\n",
    "    model_json = network.to_json()\n",
    "    with open(mdl_name, \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "    # serialize weights to HDF5\n",
    "    network.save_weights(w_name)\n",
    "    print(\"Saved \" + mdl_name + \" to disk.\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
